{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=6 color='green'><center>**Dynamic Architecture**</center></font>\n",
    "### **<center>Part 2<br/>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to train the dynmaic network architecture to learn the modality common network. The network trained here consists of five inputs (face1, face2, video1, video2, audio) and one output label, where label = 0 if audio matches with face1, video1 and label = 1 if audio matches with face2, video2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data using tensorflow data generators for efficient memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that parses the tf record\n",
    "def parser(tfRecord):\n",
    "   keys_to_features = {\n",
    "        \"img_raw1\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"img_raw2\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"video_raw1\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"video_raw2\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"audio_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\":     tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "   \n",
    "   parsed = tf.io.parse_single_example(tfRecord, keys_to_features)\n",
    "   image1 = tf.io.parse_tensor(parsed['image1_raw'], out_type=tf.float64)\n",
    "   image2 = tf.io.parse_tensor(parsed['image2_raw'], out_type=tf.float64)\n",
    "   video1 = tf.io.parse_tensor(parsed['video_raw1'], out_type=tf.float64)\n",
    "   video2 = tf.io.parse_tensor(parsed['video_raw2'], out_type=tf.float64)\n",
    "   audio = tf.io.parse_tensor(parsed['audio_raw'], out_type=tf.double)\n",
    "   audio = tf.expand_dims(audio,axis=2)\n",
    "   label = tf.cast(parsed['label'], tf.int32)\n",
    "   label = tf.one_hot(label,2)\n",
    "   return {'faceInput1':image1,'faceInput2':image2,'videoInput1':video1,'videoInput2':video2,'voiceInput':audio}, label\n",
    " \n",
    "# function to load dataset from the tfrecords file\n",
    "def get_train_data(filenames):\n",
    "  dataset = tf.data.TFRecordDataset(filenames=filenames, num_parallel_reads=40)\n",
    "  dataset = dataset.map(parser, num_parallel_calls=12)\n",
    "  dataset = dataset.batch(batch_size=32)\n",
    "  dataset = dataset.prefetch(buffer_size=2)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this path to the path file of tensorflow records\n",
    "inputDataPath = 'train2.tfrecords'\n",
    "trainDataset = get_train_data(inputDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample images and mfccs\n",
    "X_data, labels = next(iter(trainDataset))\n",
    "\n",
    "# plotting the image input\n",
    "plt.imshow(X_data['faceInput1'][0])\n",
    "plt.show()\n",
    "plt.imshow(X_data['faceInput2'][0])\n",
    "plt.show()\n",
    "\n",
    "mfcc=X_data['voiceInput'][0]\n",
    "print(f\"Shape of the mfcc co-efficients: {mfcc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the face and voice subnetworks as sequential models\n",
    "faceSubnet = keras.models.Sequential()\n",
    "\n",
    "faceSubnet.add(keras.layers.Conv2D(filters=96, kernel_size=(7,7), trainable=False, strides=(2,2), padding=\"same\", activation=\"relu\", name='flayer1i',input_shape=(224,224,3)))\n",
    "\n",
    "faceSubnet.add(keras.layers.MaxPool2D(pool_size=(2,2),padding=\"valid\",name=\"flayer1o\", trainable=False))\n",
    "\n",
    "faceSubnet.add(keras.layers.Conv2D(filters=256, kernel_size=(5,5), trainable=False, strides=(2,2), padding=\"same\", activation=\"relu\",name='flayer2i'))\n",
    "\n",
    "faceSubnet.add(keras.layers.MaxPool2D(pool_size=(2,2), trainable=False, padding=\"valid\",name='flayer2o'))\n",
    "\n",
    "faceSubnet.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), trainable=False, strides=(1,1),padding=\"same\", activation=\"relu\",name='flayer3i'))\n",
    "\n",
    "faceSubnet.add(keras.layers.MaxPool2D(pool_size=(2,2), trainable=False, padding=\"valid\",name='flayer3o'))\n",
    "\n",
    "faceSubnet.add(keras.layers.Dense(units=4096, trainable=False, activation='relu',name='flayer4i'))\n",
    "\n",
    "faceSubnet.add(keras.layers.Flatten(name='flayer40', trainable=False))\n",
    "\n",
    "faceSubnet.add(keras.layers.Dense(units=1024, trainable=False, activation='relu',name='flayer5i'))\n",
    "\n",
    "# defining the voice subnet\n",
    "\n",
    "voiceSubnet = keras.models.Sequential()\n",
    "\n",
    "voiceSubnet.add(keras.layers.Conv2D(filters=96, kernel_size=(7,7), strides=(2,2), padding=\"same\", activation=\"relu\",name='vlayer1i',input_shape=(20,130,1)))\n",
    "\n",
    "voiceSubnet.add(keras.layers.MaxPool2D(pool_size=(2,2),padding=\"valid\",name='vlayer1o'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(2,2), padding=\"same\", activation=\"relu\",name='vlayer2i'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.MaxPool2D(pool_size=(2,2), padding=\"valid\",name='vlayer2o'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1),padding=\"same\", activation=\"relu\",name='vlayer3i'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.MaxPool2D(pool_size=(1,2), padding=\"valid\",name='vlayer3o'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.Dense(units=4096, activation='relu',name='vlayer4i'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.Flatten(name='valyer4o'))\n",
    "\n",
    "voiceSubnet.add(keras.layers.Dense(units=1024, activation='relu',name='vlayer5i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3D(keras.layers.Layer):\n",
    "  # name - name given to the sequential model, used to load pre-trained weights\n",
    "  def __init__(self, filters, kernel_size, padding,name=None):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ],name=name)\n",
    "  \n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size,name):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv3D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv3D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ], name=name)\n",
    "    \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoSubnet = keras.models.Sequential(name='videoSubnet')\n",
    "\n",
    "videoSubnet.add(Conv3D(filters=32, kernel_size=(3, 7, 7), padding='same',name='videolayer1'))\n",
    "videoSubnet.add(layers.BatchNormalization())\n",
    "videoSubnet.add(layers.ReLU())\n",
    "\n",
    "videoSubnet.add(ResidualMain(64, (3,3,3),name='videolayer2'))\n",
    "videoSubnet.add(layers.ReLU())\n",
    "\n",
    "videoSubnet.add(ResidualMain(32, (3,3,3),name='videolayer3'))\n",
    "videoSubnet.add(layers.ReLU())\n",
    "\n",
    "videoSubnet.add(ResidualMain(64, (3,3,3),name='videolayer4'))\n",
    "videoSubnet.add(layers.ReLU())\n",
    "videoSubnet.add(layers.GlobalAveragePooling3D())\n",
    "videoSubnet.add(layers.Flatten())\n",
    "videoSubnet.add(layers.Dense(1024, activation='relu', name='vidoelyaer5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26168\\2371245198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfaceSubnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dynamicModel1.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mby_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvoiceSubnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dynamicModel1.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mby_name\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvideoSubnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dynamicModel1.h5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mby_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mfaceSubnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvoiceSubnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rajini bopparam\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rajini bopparam\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   3021\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 3023\u001b[1;33m                     \u001b[1;34m\"Unable to load weights saved in HDF5 format into a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3024\u001b[0m                     \u001b[1;34m\"subclassed Model which has not created its variables yet. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m                     \u001b[1;34m\"Call the Model first, then load the weights.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "# load the layer weights on the basis of layer names\n",
    "# from the model built in part 1\n",
    "faceSubnet.load_weights('dynamicModel1.h5',by_name=True)\n",
    "voiceSubnet.load_weights('dynamicModel1.h5',by_name= True)\n",
    "videoSubnet.load_weights('dynamicModel1.h5',by_name=True)\n",
    "faceSubnet.trainable=False\n",
    "voiceSubnet.trainable=False\n",
    "videoSubnet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Frames_Per_Video = 10\n",
    "faceInput1 = keras.layers.Input(shape=(224,224,3), name='faceInput1')\n",
    "faceInput2 = keras.layers.Input(shape=(224,224,3), name='faceInput2')\n",
    "videoInput1 = keras.layers.Input(shape=(Num_Frames_Per_Video, 224,224,3), name='videoInput1')\n",
    "videoInput2 = keras.layers.Input(shape=(Num_Frames_Per_Video, 224,224,3), name='videoInput2')\n",
    "voiceInput = keras.layers.Input(shape=(20,130,1), name='voiceInput')\n",
    "faceFeatures1 = faceSubnet(faceInput1)\n",
    "faceFeatures2 = faceSubnet(faceInput2)\n",
    "videoFeatures1 = videoSubnet(videoInput1)\n",
    "videoFeatures2 = videoSubnet(videoInput2)\n",
    "voiceFeatures = voiceSubnet(voiceInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedLayers = keras.layers.concatenate([faceFeatures1, faceFeatures2,videoFeatures1, videoFeatures2, voiceFeatures])\n",
    "clayer1 = keras.layers.Dense(units=2048, activation='relu',name='clayer1')\n",
    "clayer2 = keras.layers.Dense(units=512, activation='relu', name='clayer2')\n",
    "clayer3 = keras.layers.Dense(units=2, activation='sigmoid',name='clayer3')\n",
    "finalOutput = clayer3(clayer2(clayer1(concatenatedLayers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(\n",
    "  inputs = [faceInput1, faceInput2, videoInput1, videoInput2, voiceFeatures],\n",
    "  outputs = finalOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " faceInput1 (InputLayer)        [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " faceInput2 (InputLayer)        [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " videoInput1 (InputLayer)       [(None, 10, 224, 22  0           []                               \n",
      "                                4, 3)]                                                            \n",
      "                                                                                                  \n",
      " videoInput2 (InputLayer)       [(None, 10, 224, 22  0           []                               \n",
      "                                4, 3)]                                                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 1024)         207793536   ['faceInput1[0][0]',             \n",
      "                                                                  'faceInput2[0][0]']             \n",
      "                                                                                                  \n",
      " videoSubnet (Sequential)       (None, 1024)         269344      ['videoInput1[0][0]',            \n",
      "                                                                  'videoInput2[0][0]']            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 5120)         0           ['sequential[4][0]',             \n",
      "                                                                  'sequential[5][0]',             \n",
      "                                                                  'videoSubnet[2][0]',            \n",
      "                                                                  'videoSubnet[3][0]',            \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " clayer1 (Dense)                (None, 2048)         10487808    ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " clayer2 (Dense)                (None, 512)          1049088     ['clayer1[1][0]']                \n",
      "                                                                                                  \n",
      " clayer3 (Dense)                (None, 2)            1026        ['clayer2[1][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 219,600,802\n",
      "Trainable params: 11,807,202\n",
      "Non-trainable params: 207,793,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final architecture looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('dynamicModel2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
